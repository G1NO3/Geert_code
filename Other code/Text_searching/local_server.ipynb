{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import socket\n",
    "import json\n",
    "from multiprocessing import Pool,Process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  level_1      words\n",
      "0          1        0  Quarterly\n",
      "1          1        1     profit\n",
      "2          1        2         US\n",
      "3          1        3     medium\n",
      "4          1        4      giant\n",
      "...      ...      ...        ...\n",
      "507047  2225     1636     Online\n",
      "507048  2225     1637       game\n",
      "507049  2225     1638      ahhhh\n",
      "507050  2225     1639        day\n",
      "507051  2225     1640        LOL\n",
      "\n",
      "[507052 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "English_f=pd.read_csv('all_news.csv')\n",
    "\n",
    "\n",
    "stoplist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', \n",
    "            'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', \n",
    "            'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', \n",
    "            'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', \n",
    "            'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', \n",
    "            'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', \n",
    "            'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', \n",
    "            'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', \n",
    "            'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', \n",
    "            'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', \n",
    "            'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', \n",
    "            'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which']\n",
    "\n",
    "#分词\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "def wordcount(news):\n",
    "    freq=[]\n",
    "    news=re.sub(r'\\W+|\\d+',' ',news)\n",
    "    for word in news.split():\n",
    "        if word not in stoplist:\n",
    "            freq.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(freq)\n",
    "English_f['word']=English_f['body'].apply(wordcount)\n",
    "words=English_f['word'].str.split(' ',expand=True).stack().rename('words').reset_index()\n",
    "words['level_0']=words['level_0']+1\n",
    "words.columns=['id','level_1','words']\n",
    "print(words)\n",
    "\n",
    "#计算TF\n",
    "def counttf(x):\n",
    "    l=len(x)\n",
    "    p=x['words'].value_counts()\n",
    "    d={'words':p.index,\n",
    "       'TF':p.values/l}\n",
    "    p_2d=pd.DataFrame(data=d)\n",
    "\n",
    "    r=pd.merge(x,p_2d,on='words')\n",
    "    return r\n",
    "\n",
    "TF_data=words.groupby('id').apply(counttf)\n",
    "#统计完频率之后 去除重复词\n",
    "TF_data.drop_duplicates(subset=['id','words'],inplace=True)\n",
    "\n",
    "#计算IDF\n",
    "doc_num=len(English_f)\n",
    "def countidf(x):\n",
    "    df_t=len(x['id'].value_counts())\n",
    "    return np.log(doc_num/df_t)\n",
    "\n",
    "IDF_data=words.groupby('words').apply(countidf).reset_index()\n",
    "IDF_data.columns=['words','IDF']\n",
    "tf_idf=pd.merge(TF_data,IDF_data)\n",
    "tf_idf['TF-IDF']=tf_idf['TF']*tf_idf['IDF']\n",
    "tf_idf.to_csv('tfidf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化为词向量\n",
    "print(tf_idf['words'])\n",
    "a=list(set(tf_idf['words']))\n",
    "k=len(a)\n",
    "word_list=dict(zip(a,list(range(k))))\n",
    "wordlst=pd.Series(word_list)\n",
    "\n",
    "def wordtovec(x):\n",
    "    vec=np.zeros(k,)\n",
    "    for i in range(x.shape[0]):\n",
    "        idx=word_list[x.iloc[i]['words']]\n",
    "        vec[idx]=x.iloc[i]['TF-IDF']\n",
    "    return vec\n",
    "newdata=tf_idf.groupby('id').apply(wordtovec)\n",
    "print(newdata)\n",
    "\n",
    "#降维\n",
    "new_data=np.vstack(newdata.values)\n",
    "\n",
    "model=PCA(n_components=300)\n",
    "dev=model.fit_transform(new_data)\n",
    "\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算各个文章之间的相似度\n",
    "for i in range(dev.shape[0]):\n",
    "    dev[i]/=np.linalg.norm(dev[i])\n",
    "cos_sim=dev@dev.T\n",
    "\n",
    "print(cos_sim.shape)\n",
    "cos_sim=cos_sim-np.eye(2225)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用tf-idf矩阵，计算词之间的相似度\n",
    "new_data=np.load('tf_idf.npy')\n",
    "matrix=new_data.T@new_data\n",
    "\n",
    "x=matrix.shape[0]\n",
    "idxar=np.argsort(matrix,axis=1)[:,-4:-1]#取相似度排序前三个为模糊词\n",
    "synonym=word_list\n",
    "for word in word_list:\n",
    "    idxlst=idxar[word_list[word]]\n",
    "    lst=[]\n",
    "    for k in idxlst:\n",
    "        lst.append(a[k])#a是前面提到的词典，它和word_list互为补充，使查询时间缩短到O(1)\n",
    "    synonym[word]=lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将之前各步结果保存下来，ndarray用.npy保存，dict用json保存\n",
    "np.save('tf_idf.npy',new_data)\n",
    "np.save('cos_sim.npy',cos_sim)\n",
    "np.save('dev.npy',dev)\n",
    "f=open('wordlist.txt','w')\n",
    "f.write(json.dumps(word_list))\n",
    "f.close()\n",
    "f=open('synonym.txt','w')#将模糊词典保存下来\n",
    "f.write(json.dumps(synonym))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###以下为测试部分，其中各步运算结果已经保存为文件，因此加载比较快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import socket\n",
    "import json\n",
    "from multiprocessing import Pool,Process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "English_f=pd.read_csv('all_news.csv')\n",
    "cos_sim=np.load('cos_sim.npy')\n",
    "with open('wordlist.txt') as f:#空间换时间的哈希表\n",
    "    word_list=json.load(f)\n",
    "tf_idf=pd.read_csv('tfidf.csv')\n",
    "topics=np.array(English_f['topic'])\n",
    "titles=np.array(English_f['title'])\n",
    "with open('synonym.txt') as f:#模糊词检索表\n",
    "    synonym=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalServer(object):\n",
    "    def __init__(self, host, port):\n",
    "        self.address = (host, port)\n",
    "    #这一个函数依据输入的word列表，输出所有匹配的articles  \n",
    "    #传参包括connection和word列表，最后直接在此函数中将结果返回，做到了并发处理\n",
    "    def lookupfor(self,words,conn):\n",
    "        lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "        for i in range(len(words)):\n",
    "            words[i]=lemmatizer.lemmatize(words[i])#使用lemmatizer进行词性还原\n",
    "            words+=synonym[words[i]]#将模糊词添加到列表中\n",
    "        global topics\n",
    "        global titles\n",
    "        k=len(words)\n",
    "        l=len(topics)\n",
    "        returnlist=[]\n",
    "        #首先检索主题和题目，这里认为二者重要性相同，如果又出现了一次，认为其重要性更高，排在前面\n",
    "        for j in range(k):\n",
    "            for i in range(l):\n",
    "                if topics[i]==words[j]:\n",
    "                    if i in returnlist:\n",
    "                        returnlist[returnlist.index[i]],returnlist[0]=returnlist[0],returnlist[returnlist.index[i]]\n",
    "                    else:\n",
    "                        returnlist.append(i)\n",
    "                if titles[i]==words[j]:\n",
    "                    if i in returnlist:\n",
    "                        returnlist[returnlist.index[i]],returnlist[0]=returnlist[0],returnlist[returnlist.index[i]]\n",
    "                    else:\n",
    "                        returnlist.append(i)\n",
    "        #其次检索与已有文章相似度最高的一篇文章，依次放入returnlist\n",
    "        for num in returnlist:\n",
    "            idx=np.argmax(cos_sim[num])\n",
    "            if idx in returnlist:\n",
    "                returnlist[returnlist.index[idx]],returnlist[0]=returnlist[0],returnlist[returnlist.index[idx]]\n",
    "            else:\n",
    "                returnlist.append(idx)\n",
    "        #最后按关键字进行查询，按TF-IDF由高到低依次输出\n",
    "        for j in range(k):\n",
    "            if words[j] in word_list:\n",
    "                df=tf_idf[tf_idf['words']==words[j]]\n",
    "                for i in df['id']:\n",
    "                    if i not in returnlist:\n",
    "                        returnlist.append(i-1)\n",
    "        articles=[]\n",
    "        for k in returnlist:\n",
    "            articles.append(tuple(English_f.iloc[k,0:2]))\n",
    "\n",
    "        conn.send(json.dumps(articles).encode('utf-8'))#将结果转化为json字符串后返回客户端\n",
    "        return articles\n",
    "    \n",
    "    def run(self):\n",
    "        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        server.bind(self.address)\n",
    "        i=0\n",
    "        server.listen(5)\n",
    "        while True:\n",
    "            conn,add=server.accept()\n",
    "            print(f'Connected to {add}')\n",
    "            \n",
    "            words=json.loads(conn.recv(1024).decode('utf-8'))#接收传参\n",
    "            if 'break_000' in words:                        #此处认为break_000为结束符\n",
    "                break\n",
    "            \n",
    "            print(words)    \n",
    "            p=Thread(target=self.lookupfor,args=(words,conn))#多线程实现并发处理\n",
    "            p.start()\n",
    "        p.join()\n",
    "        server.close()\n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行服务器端\n",
    "启动服务器之后，在run.ipynb中运行客户端图形界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ('127.0.0.1', 62499)\n",
      "['science']\n"
     ]
    }
   ],
   "source": [
    "server = LocalServer(\"127.0.0.1\", 1234)\n",
    "server.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc8f97a2891ced11e6ce787110d7c4e115bf272f555f38ca53035080013eaa71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
